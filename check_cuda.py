"""
CUDA é…ç½®æª¢æŸ¥å·¥å…·
è¨ºæ–· PyTorch å’Œ CUDA å…¼å®¹æ€§å•é¡Œ
"""
import sys
import subprocess

def print_section(title):
    """æ‰“å°åˆ†éš”ç·š"""
    print(f"\n{'='*60}")
    print(f"{title}")
    print(f"{'='*60}")

def check_nvidia_smi():
    """æª¢æŸ¥ nvidia-smi"""
    print_section("æª¢æŸ¥ NVIDIA GPU é©…å‹•")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print("âœ“ NVIDIA é©…å‹•å·²å®‰è£")
            # æå– CUDA ç‰ˆæœ¬
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version' in line:
                    print(f"  {line.strip()}")
                if '|' in line and 'MiB' in line and not 'Processes' in line:
                    print(f"  {line.strip()}")
            return True
        else:
            print("âœ— nvidia-smi ç„¡æ³•é‹è¡Œ")
            return False
    except FileNotFoundError:
        print("âœ— æœªæ‰¾åˆ° nvidia-smi (å¯èƒ½æ²’æœ‰å®‰è£ NVIDIA é©…å‹•)")
        return False
    except Exception as e:
        print(f"âœ— æª¢æŸ¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def check_pytorch():
    """æª¢æŸ¥ PyTorch é…ç½®"""
    print_section("æª¢æŸ¥ PyTorch é…ç½®")
    
    try:
        import torch
        print(f"âœ“ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        
        # CUDA å¯ç”¨æ€§
        cuda_available = torch.cuda.is_available()
        print(f"{'âœ“' if cuda_available else 'âœ—'} CUDA å¯ç”¨: {cuda_available}")
        
        if cuda_available:
            # CUDA ç‰ˆæœ¬
            print(f"  PyTorch CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            
            # cuDNN ç‰ˆæœ¬
            if torch.backends.cudnn.is_available():
                print(f"  cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            
            # GPU æ•¸é‡å’Œåç¨±
            gpu_count = torch.cuda.device_count()
            print(f"  æª¢æ¸¬åˆ° {gpu_count} å€‹ GPU:")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_capability = torch.cuda.get_device_capability(i)
                print(f"    GPU {i}: {gpu_name}")
                print(f"      è¨ˆç®—èƒ½åŠ›: {gpu_capability[0]}.{gpu_capability[1]}")
                
                # å…§å­˜ä¿¡æ¯
                try:
                    total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    print(f"      é¡¯å­˜: {total_memory:.2f} GB")
                except:
                    pass
        else:
            print("\nâš ï¸ CUDA ä¸å¯ç”¨çš„å¯èƒ½åŸå› :")
            print("  1. PyTorch CPU ç‰ˆæœ¬ (æ²’æœ‰ CUDA æ”¯æŒ)")
            print("  2. PyTorch CUDA ç‰ˆæœ¬èˆ‡ç³»çµ± CUDA ä¸å…¼å®¹")
            print("  3. æ²’æœ‰ NVIDIA GPU")
            print("  4. NVIDIA é©…å‹•æœªæ­£ç¢ºå®‰è£")
        
        return cuda_available
        
    except ImportError:
        print("âœ— PyTorch æœªå®‰è£")
        print("  å®‰è£: pip install torch torchvision")
        return False

def test_cuda_operation():
    """æ¸¬è©¦ CUDA æ“ä½œ"""
    print_section("æ¸¬è©¦ CUDA æ“ä½œ")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âŠ— è·³é (CUDA ä¸å¯ç”¨)")
            return False
        
        print("æ¸¬è©¦ç°¡å–®çš„å¼µé‡æ“ä½œ...")
        try:
            # å‰µå»ºå¼µé‡ä¸¦ç§»åˆ° GPU
            x = torch.randn(3, 3).cuda()
            y = torch.randn(3, 3).cuda()
            z = x + y
            z.cpu()
            print("âœ“ åŸºæœ¬å¼µé‡æ“ä½œæˆåŠŸ")
            
            # æ¸¬è©¦å·ç©æ“ä½œ
            print("æ¸¬è©¦å·ç©æ“ä½œ...")
            conv = torch.nn.Conv2d(3, 64, 3, padding=1).cuda()
            input_tensor = torch.randn(1, 3, 224, 224).cuda()
            output = conv(input_tensor)
            print("âœ“ å·ç©æ“ä½œæˆåŠŸ")
            
            return True
            
        except RuntimeError as e:
            print(f"âœ— CUDA æ“ä½œå¤±æ•—: {e}")
            if "no kernel image is available" in str(e):
                print("\nâš ï¸ é€™æ˜¯ PyTorch/CUDA ç‰ˆæœ¬ä¸å…¼å®¹çš„å…¸å‹éŒ¯èª¤!")
                print("   éœ€è¦é‡æ–°å®‰è£åŒ¹é…çš„ PyTorch ç‰ˆæœ¬")
            return False
            
    except Exception as e:
        print(f"âœ— æ¸¬è©¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def recommend_solution(has_gpu, pytorch_cuda):
    """æ¨è–¦è§£æ±ºæ–¹æ¡ˆ"""
    print_section("å»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ")
    
    if not has_gpu:
        print("âš ï¸ æœªæª¢æ¸¬åˆ° NVIDIA GPU")
        print("\né¸é … 1: ä½¿ç”¨ CPU è¨“ç·´")
        print("  python train.py --gpu_id -1 --batch_size 1")
        print("  æˆ–é‹è¡Œ: train_cpu.bat")
        print("\né¸é … 2: ä½¿ç”¨é›²ç«¯ GPU (Google Colab, AWS, Azure ç­‰)")
        
    elif not pytorch_cuda:
        print("âš ï¸ æœ‰ GPU ä½† PyTorch CUDA ä¸å¯ç”¨")
        print("\nå¯èƒ½åŸå› :")
        print("  1. å®‰è£äº† CPU ç‰ˆæœ¬çš„ PyTorch")
        print("  2. PyTorch CUDA ç‰ˆæœ¬èˆ‡ç³»çµ±ä¸å…¼å®¹")
        
        print("\nè§£æ±ºæ–¹æ¡ˆ: é‡æ–°å®‰è£ PyTorch")
        print("\né¦–å…ˆå¸è¼‰:")
        print("  pip uninstall torch torchvision")
        
        print("\nç„¶å¾Œæ ¹æ“šæ‚¨çš„ CUDA ç‰ˆæœ¬å®‰è£:")
        print("\nCUDA 11.8 (æ¨è–¦):")
        print("  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
        
        print("\nCUDA 12.1:")
        print("  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121")
        
        print("\nCUDA 12.4+:")
        print("  pip install torch torchvision")
        
    else:
        print("âœ“ PyTorch å’Œ CUDA é…ç½®æ­£å¸¸")
        print("\nå¦‚æœä»ç„¶é‡åˆ°éŒ¯èª¤ï¼Œå¯èƒ½éœ€è¦:")
        print("  1. æ›´æ–° NVIDIA é©…å‹•")
        print("  2. é‡æ–°å®‰è£ PyTorch")
        print("  3. æª¢æŸ¥ä»£ç¢¼ä¸­çš„éŒ¯èª¤ (å¦‚æ‹¼å¯«éŒ¯èª¤)")

def main():
    print("\n" + "="*60)
    print("HDRSL CUDA é…ç½®è¨ºæ–·å·¥å…·")
    print("="*60)
    
    # æª¢æŸ¥æ­¥é©Ÿ
    has_gpu = check_nvidia_smi()
    pytorch_cuda = check_pytorch()
    
    if pytorch_cuda:
        cuda_works = test_cuda_operation()
    else:
        cuda_works = False
    
    # ç¸½çµ
    print_section("è¨ºæ–·ç¸½çµ")
    print(f"NVIDIA GPU é©…å‹•:  {'âœ“ å·²å®‰è£' if has_gpu else 'âœ— æœªæª¢æ¸¬åˆ°'}")
    print(f"PyTorch CUDA:     {'âœ“ å¯ç”¨' if pytorch_cuda else 'âœ— ä¸å¯ç”¨'}")
    print(f"CUDA æ“ä½œæ¸¬è©¦:    {'âœ“ é€šé' if cuda_works else 'âœ— å¤±æ•—æˆ–è·³é'}")
    
    # æ¨è–¦è§£æ±ºæ–¹æ¡ˆ
    recommend_solution(has_gpu, pytorch_cuda)
    
    print("\n" + "="*60)
    if cuda_works:
        print("ğŸ‰ ç³»çµ±é…ç½®æ­£å¸¸ï¼Œå¯ä»¥é–‹å§‹è¨“ç·´!")
        print("   é‹è¡Œ: train.bat")
    else:
        print("âš ï¸ éœ€è¦ä¿®å¾©é…ç½®å¾Œæ‰èƒ½ä½¿ç”¨ GPU è¨“ç·´")
        print("   è©³ç´°èªªæ˜è«‹æŸ¥çœ‹: CUDA_FIX_GUIDE.txt")
    print("="*60)

if __name__ == "__main__":
    main()
